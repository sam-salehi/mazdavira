[{"title":"Attention Is All You Need","authors":["Ashish Vaswani","Noam Shazeer","Niki Parmar","Jakob Uszkoreit","Llion Jones","Aidan N. Gomez","Łukasz Kaiser","Illia Polosukhin"],"institutions":["Google Brain","Google Research","University of Toronto"],"pub_year":2017,"arxiv":"1706.03762v7","doi":"null","referencing_count":40,"referenced_count":null,"pdf_link":"http://arxiv.org/pdf/1706.03762v7"},{"title":"Layer normalization","authors":["Jimmy Lei Ba","Jamie Ryan Kiros","Geoffrey E Hinton"],"institutions":[],"pub_year":2016,"arxiv":"1607.06450","doi":null,"referencing_count":null,"referenced_count":9718,"pdf_link":"http://arxiv.org/pdf/1607.06450v1"},{"title":"Neural machine translation by jointly learning to align and translate","authors":["Dzmitry Bahdanau","Kyunghyun Cho","Yoshua Bengio"],"institutions":[],"pub_year":2014,"arxiv":"1409.0473","doi":null,"referencing_count":null,"referenced_count":26403,"pdf_link":"http://arxiv.org/pdf/1409.0473v7"},{"title":"Massive exploration of neural machine translation architectures","authors":["Denny Britz","Anna Goldie","Minh-Thang Luong","Quoc V. Le"],"institutions":[],"pub_year":2017,"arxiv":"1703.03906","doi":null,"referencing_count":null,"referenced_count":510,"pdf_link":"http://arxiv.org/pdf/1703.03906v2"},{"title":"Long short-term memory-networks for machine reading","authors":["Jianpeng Cheng","Li Dong","Mirella Lapata"],"institutions":[],"pub_year":2016,"arxiv":"1601.06733","doi":null,"referencing_count":null,"referenced_count":1073,"pdf_link":"http://arxiv.org/pdf/1601.06733v7"},{"title":"Learning phrase representations using rnn encoder-decoder for statistical machine translation","authors":["Kyunghyun Cho","Bart van Merrienboer","Caglar Gulcehre","Fethi Bougares","Holger Schwenk","Yoshua Bengio"],"institutions":[],"pub_year":2014,"arxiv":"1406.1078","doi":null,"referencing_count":null,"referenced_count":22192,"pdf_link":"http://arxiv.org/pdf/1406.1078v3"},{"title":"Xception: Deep learning with depthwise separable convolutions","authors":["Francois Chollet"],"institutions":[],"pub_year":2016,"arxiv":"1610.02357","doi":null,"referencing_count":null,"referenced_count":13456,"pdf_link":"http://arxiv.org/pdf/1610.02357v3"},{"title":"Empirical evaluation of gated recurrent neural networks on sequence modeling","authors":["Junyoung Chung","Çaglar Gülçehre","Kyunghyun Cho","Yoshua Bengio"],"institutions":[],"pub_year":2014,"arxiv":"1412.3555","doi":null,"referencing_count":null,"referenced_count":11962,"pdf_link":"http://arxiv.org/pdf/1412.3555v1"},{"title":"Recurrent neural network grammars","authors":["Chris Dyer","Adhiguna Kuncoro","Miguel Ballesteros","Noah A. Smith"],"institutions":[],"pub_year":2016,"arxiv":null,"doi":null,"referencing_count":null,"referenced_count":null,"pdf_link":null},{"title":"Convolutional sequence to sequence learning","authors":["Jonas Gehring","Michael Auli","David Grangier","Denis Yarats","Yann N. Dauphin"],"institutions":[],"pub_year":2017,"arxiv":"1705.03122v2","doi":null,"referencing_count":null,"referenced_count":null,"pdf_link":"http://arxiv.org/pdf/1705.03122v2"},{"title":"Generating sequences with recurrent neural networks","authors":["Alex Graves"],"institutions":[],"pub_year":2013,"arxiv":"1308.0850","doi":null,"referencing_count":null,"referenced_count":3930,"pdf_link":"http://arxiv.org/pdf/1308.0850v5"},{"title":"Deep residual learning for image recognition","authors":["Kaiming He","Xiangyu Zhang","Shaoqing Ren","Jian Sun"],"institutions":[],"pub_year":2016,"arxiv":null,"doi":null,"referencing_count":null,"referenced_count":null,"pdf_link":null},{"title":"Gradient flow in recurrent nets: the difficulty of learning long-term dependencies","authors":["Sepp Hochreiter","Yoshua Bengio","Paolo Frasconi","Jürgen Schmidhuber"],"institutions":[],"pub_year":2001,"arxiv":null,"doi":null,"referencing_count":null,"referenced_count":null,"pdf_link":null},{"title":"Long short-term memory","authors":["Sepp Hochreiter","Jürgen Schmidhuber"],"institutions":[],"pub_year":1997,"arxiv":null,"doi":null,"referencing_count":null,"referenced_count":null,"pdf_link":null},{"title":"Self-training PCFG grammars with latent annotations across languages","authors":["Zhongqiang Huang","Mary Harper"],"institutions":[],"pub_year":2009,"arxiv":null,"doi":null,"referencing_count":null,"referenced_count":null,"pdf_link":null},{"title":"Exploring the limits of language modeling","authors":["Rafal Jozefowicz","Oriol Vinyals","Mike Schuster","Noam Shazeer","Yonghui Wu"],"institutions":[],"pub_year":2016,"arxiv":"1602.02410","doi":null,"referencing_count":null,"referenced_count":1120,"pdf_link":"http://arxiv.org/pdf/1602.02410v2"},{"title":"Can active memory replace attention?","authors":["Łukasz Kaiser","Samy Bengio"],"institutions":[],"pub_year":2016,"arxiv":null,"doi":null,"referencing_count":null,"referenced_count":null,"pdf_link":null},{"title":"Neural GPUs learn algorithms","authors":["Łukasz Kaiser","Ilya Sutskever"],"institutions":[],"pub_year":2016,"arxiv":null,"doi":null,"referencing_count":null,"referenced_count":null,"pdf_link":null},{"title":"Neural machine translation in linear time","authors":["Nal Kalchbrenner","Lasse Espeholt","Karen Simonyan","Aaron van den Oord","Alex Graves","Koray Kavukcuoglu"],"institutions":[],"pub_year":2017,"arxiv":"1610.10099v2","doi":null,"referencing_count":null,"referenced_count":null,"pdf_link":"http://arxiv.org/pdf/1610.10099v2"},{"title":"Structured attention networks","authors":["Yoon Kim","Carl Denton","Luong Hoang","Alexander M. Rush"],"institutions":[],"pub_year":2017,"arxiv":null,"doi":null,"referencing_count":null,"referenced_count":null,"pdf_link":null},{"title":"Adam: A method for stochastic optimization","authors":["Diederik Kingma","Jimmy Ba"],"institutions":[],"pub_year":2015,"arxiv":null,"doi":null,"referencing_count":null,"referenced_count":null,"pdf_link":null},{"title":"Factorization tricks for LSTM networks","authors":["Oleksii Kuchaiev","Boris Ginsburg"],"institutions":[],"pub_year":2017,"arxiv":"1703.10722","doi":null,"referencing_count":null,"referenced_count":112,"pdf_link":"http://arxiv.org/pdf/1703.10722v3"},{"title":"A structured self-attentive sentence embedding","authors":["Zhouhan Lin","Minwei Feng","Cicero Nogueira dos Santos","Mo Yu","Bing Xiang","Bowen Zhou","Yoshua Bengio"],"institutions":[],"pub_year":2017,"arxiv":"1703.03130","doi":null,"referencing_count":null,"referenced_count":2076,"pdf_link":"http://arxiv.org/pdf/1703.03130v1"},{"title":"Multi-task sequence to sequence learning","authors":["Minh-Thang Luong","Quoc V. Le","Ilya Sutskever","Oriol Vinyals","Lukasz Kaiser"],"institutions":[],"pub_year":2015,"arxiv":"1511.06114","doi":null,"referencing_count":null,"referenced_count":795,"pdf_link":"http://arxiv.org/pdf/1511.06114v4"},{"title":"Effective approaches to attention-based neural machine translation","authors":["Minh-Thang Luong","Hieu Pham","Christopher D Manning"],"institutions":[],"pub_year":2015,"arxiv":"1508.04025","doi":null,"referencing_count":null,"referenced_count":7788,"pdf_link":"http://arxiv.org/pdf/1508.04025v5"},{"title":"Building a large annotated corpus of english: The penn treebank","authors":["Mitchell P Marcus","Mary Ann Marcinkiewicz","Beatrice Santorini"],"institutions":[],"pub_year":1993,"arxiv":null,"doi":null,"referencing_count":null,"referenced_count":null,"pdf_link":null},{"title":"Effective self-training for parsing","authors":["David McClosky","Eugene Charniak","Mark Johnson"],"institutions":[],"pub_year":2006,"arxiv":null,"doi":null,"referencing_count":null,"referenced_count":null,"pdf_link":null},{"title":"A decomposable attention model","authors":["Ankur Parikh","Oscar Täckström","Dipanjan Das","Jakob Uszkoreit"],"institutions":[],"pub_year":2016,"arxiv":null,"doi":null,"referencing_count":null,"referenced_count":null,"pdf_link":null},{"title":"A deep reinforced model for abstractive summarization","authors":["Romain Paulus","Caiming Xiong","Richard Socher"],"institutions":[],"pub_year":2017,"arxiv":"1705.04304","doi":null,"referencing_count":null,"referenced_count":1509,"pdf_link":"http://arxiv.org/pdf/1705.04304v3"},{"title":"Learning accurate, compact, and interpretable tree annotation","authors":["Slav Petrov","Leon Barrett","Romain Thibaux","Dan Klein"],"institutions":[],"pub_year":2006,"arxiv":null,"doi":null,"referencing_count":null,"referenced_count":null,"pdf_link":null},{"title":"Using the output embedding to improve language models","authors":["Ofir Press","Lior Wolf"],"institutions":[],"pub_year":2016,"arxiv":"1608.05859","doi":null,"referencing_count":null,"referenced_count":713,"pdf_link":"http://arxiv.org/pdf/1608.05859v3"},{"title":"Neural machine translation of rare words with subword units","authors":["Rico Sennrich","Barry Haddow","Alexandra Birch"],"institutions":[],"pub_year":2015,"arxiv":"1508.07909","doi":null,"referencing_count":null,"referenced_count":7358,"pdf_link":"http://arxiv.org/pdf/1508.07909v5"},{"title":"Outrageously large neural networks: The sparsely-gated mixture-of-experts layer","authors":["Noam Shazeer","Azalia Mirhoseini","Krzysztof Maziarz","Andy Davis","Quoc Le","Geoffrey Hinton","Jeff Dean"],"institutions":[],"pub_year":2017,"arxiv":"1701.06538","doi":null,"referencing_count":null,"referenced_count":2162,"pdf_link":"http://arxiv.org/pdf/1701.06538v1"},{"title":"Dropout: a simple way to prevent neural networks from overfitting","authors":["Nitish Srivastava","Geoffrey E Hinton","Alex Krizhevsky","Ilya Sutskever","Ruslan Salakhutdinov"],"institutions":[],"pub_year":2014,"arxiv":null,"doi":null,"referencing_count":null,"referenced_count":null,"pdf_link":null},{"title":"End-to-end memory networks","authors":["Sainbayar Sukhbaatar","Arthur Szlam","Jason Weston","Rob Fergus"],"institutions":[],"pub_year":2015,"arxiv":null,"doi":null,"referencing_count":null,"referenced_count":null,"pdf_link":null},{"title":"Sequence to sequence learning with neural networks","authors":["Ilya Sutskever","Oriol Vinyals","Quoc VV Le"],"institutions":[],"pub_year":2014,"arxiv":null,"doi":null,"referencing_count":null,"referenced_count":null,"pdf_link":null},{"title":"Rethinking the inception architecture for computer vision","authors":["Christian Szegedy","Vincent Vanhoucke","Sergey Ioffe","Jonathon Shlens","Zbigniew Wojna"],"institutions":[],"pub_year":2015,"arxiv":"1512.00567","doi":null,"referencing_count":null,"referenced_count":25923,"pdf_link":"http://arxiv.org/pdf/1512.00567v3"},{"title":"Grammar as a foreign language","authors":["Vinyals & Kaiser","Koo","Petrov","Sutskever","Hinton"],"institutions":[],"pub_year":2015,"arxiv":null,"doi":null,"referencing_count":null,"referenced_count":null,"pdf_link":null},{"title":"Google’s neural machine translation system: Bridging the gap between human and machine translation","authors":["Yonghui Wu","Mike Schuster","Zhifeng Chen","Quoc V Le","Mohammad Norouzi","Wolfgang Macherey","Maxim Krikun","Yuan Cao","Qin Gao","Klaus Macherey","et al"],"institutions":[],"pub_year":2016,"arxiv":"1609.08144","doi":null,"referencing_count":null,"referenced_count":6575,"pdf_link":"http://arxiv.org/pdf/1609.08144v2"},{"title":"Deep recurrent models with fast-forward connections for neural machine translation","authors":["Jie Zhou","Ying Cao","Xuguang Wang","Peng Li","Wei Xu"],"institutions":[],"pub_year":2016,"arxiv":"1606.04199","doi":null,"referencing_count":null,"referenced_count":213,"pdf_link":"http://arxiv.org/pdf/1606.04199v3"},{"title":"Fast and accurate shift-reduce constituent parsing","authors":["Muhua Zhu","Yue Zhang","Wenliang Chen","Min Zhang","Jingbo Zhu"],"institutions":[],"pub_year":2013,"arxiv":null,"doi":null,"referencing_count":null,"referenced_count":null,"pdf_link":null}]